import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from xgboost import XGBRegressor

# ==========================================
# 1. è¨­å®šèˆ‡è³‡æ–™è¼‰å…¥
# ==========================================
RANDOM_STATE = 42
RESULTS_DIR = 'results'
os.makedirs(RESULTS_DIR, exist_ok=True)

print("ğŸ“š æ­£åœ¨è¼‰å…¥è³‡æ–™...")
df = pd.read_csv('FINAL_MODEL_DATA_WITH_FEATURES.csv', parse_dates=['rent_time'])

# ==========================================
# 2. è³‡æ–™å‰è™•ç†èˆ‡åˆ‡åˆ†
# ==========================================
print("âœ‚ï¸  æ­£åœ¨åˆ‡åˆ†è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†...")

# æº–å‚™ç‰¹å¾µèˆ‡ç›®æ¨™
target = df['rent_count']
features = df.drop(columns=['rent_count', 'rent_time'], errors='ignore')

# ä¾æ™‚é–“åˆ‡åˆ† (3æœˆç‚ºæ¸¬è©¦é›†)
test_mask = df['rent_time'].dt.month == 3
X_train = features[~test_mask]
y_train = target[~test_mask]
X_test = features[test_mask]
y_test = target[test_mask]

print(f"   è¨“ç·´é›†: {len(X_train)} ç­†, æ¸¬è©¦é›†: {len(X_test)} ç­†")

# å»ºç«‹é è™•ç†å™¨ (æ•¸å€¼è£œä¸­ä½æ•¸+æ¨™æº–åŒ–, é¡åˆ¥è£œçœ¾æ•¸+OneHot)
numeric_features = X_train.select_dtypes(include=['number']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), numeric_features),
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ]), categorical_features)
    ]
)

# ==========================================
# 3. æ¨¡å‹å®šç¾©
# ==========================================
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(
        n_estimators=200,
        max_depth=12,
        min_samples_leaf=2,
        n_jobs=-1,
        random_state=RANDOM_STATE
    ),
    "XGBoost": XGBRegressor(
        n_estimators=200,
        learning_rate=0.05,
        max_depth=4,
        subsample=0.8,
        colsample_bytree=0.8,
        objective="reg:squarederror",
        n_jobs=-1,
        tree_method="hist",
        random_state=RANDOM_STATE
    )
}

# ==========================================
# 4. è¨“ç·´èˆ‡è©•ä¼°
# ==========================================
results = []

print("\nğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹...")
for name, model in models.items():
    print(f"   æ­£åœ¨è¨“ç·´ {name}...")
    
    # å»ºç«‹ä¸¦è¨“ç·´ Pipeline
    pipeline = Pipeline([
        ('preprocess', preprocessor),
        ('model', model)
    ])
    pipeline.fit(X_train, y_train)
    
    # å„²å­˜æ¨¡å‹
    model_dir = 'model'
    os.makedirs(model_dir, exist_ok=True)
    safe_name = name.replace(" ", "_").lower()
    model_path = os.path.join(model_dir, f'{safe_name}_model.joblib')
    joblib.dump(pipeline, model_path)
    print(f"   ğŸ’¾ æ¨¡å‹å·²å„²å­˜è‡³: {model_path}")

    # é æ¸¬
    y_pred = pipeline.predict(X_test)
    
    # è¨ˆç®—æŒ‡æ¨™
    mae = mean_absolute_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred) ** 0.5
    r2 = r2_score(y_test, y_pred)
    
    results.append({
        'Model': name,
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2
    })

# è½‰ç‚º DataFrame ä¸¦é¡¯ç¤º
metrics_df = pd.DataFrame(results).sort_values('MAE')
print("\nğŸ“Š æ¨¡å‹è©•ä¼°çµæœ:")
print(metrics_df.to_string(index=False, float_format=lambda x: f"{x:.3f}"))

# å„²å­˜çµæœ
metrics_path = os.path.join(RESULTS_DIR, 'baseline_model_metrics.csv')
metrics_df.to_csv(metrics_path, index=False)
print(f"\nğŸ’¾ è©•ä¼°è¡¨å·²å„²å­˜è‡³: {metrics_path}")

# ==========================================
# 5. ç¹ªè£½æ¯”è¼ƒåœ–è¡¨
# ==========================================
print("ğŸ¨ æ­£åœ¨ç¹ªè£½æ¯”è¼ƒåœ–è¡¨...")
sns.set_theme(style="whitegrid")

# æº–å‚™ç¹ªåœ–è³‡æ–™
long_df = metrics_df.melt(id_vars="Model", var_name="Metric", value_name="Score")
error_df = long_df[long_df["Metric"].isin(["MAE", "RMSE"])]
r2_df = long_df[long_df["Metric"] == "R2"]

fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# å·¦åœ–: MAE & RMSE
sns.barplot(data=error_df, x="Metric", y="Score", hue="Model", ax=axes[0], palette="viridis")
axes[0].set_title("Error Metrics (Lower is Better)")
axes[0].set_ylabel("Score")

# å³åœ–: R2
sns.barplot(data=r2_df, x="Metric", y="Score", hue="Model", ax=axes[1], palette="viridis")
axes[1].set_title("R2 Score (Higher is Better)")
axes[1].set_ylabel("Score")
axes[1].set_ylim(0, 1.0)

# æ¨™è¨»æ•¸å€¼
for ax in axes:
    for container in ax.containers:
        ax.bar_label(container, fmt='%.2f', padding=3)

plt.tight_layout()
figure_path = os.path.join(RESULTS_DIR, 'baseline_model_comparison.png')
plt.savefig(figure_path, dpi=300)
print(f"ğŸ–¼ï¸  æ¯”è¼ƒåœ–å·²å„²å­˜è‡³: {figure_path}")
print("\nğŸ‰ åŸ·è¡Œå®Œç•¢ï¼")
